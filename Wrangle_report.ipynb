{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdfd9dd",
   "metadata": {},
   "source": [
    "# Report on Data Wrangling\n",
    "<P>Data Wrangling is all about gathering,assessing and cleaning data.In this project which is my second project under Udacity, I was able to gather,assess and clean my data set.</P>\n",
    "\n",
    "In this project,we were required to gather data from three different sources. These data are twitter_enhanced_archive.csv,image_prediction.tsv and additional data from twitter.Twitter_enhanced_archive.csv link was provided in the Udacity classroom, thus i downloaded the file manually and stored it in my project folder.This is a csv file which means its separators are commas.On my jupyter notebook, i read my file into a pandas dataframe using the following code \"pd.read_csv('twitter_enhanced_archive.csv) and called it to see if my data is gathered.\n",
    "The second dataset is the image_prediction.tsv data.This is tsv file which means that its delimiters are tabs.I downloaded this data programmatically using the requests library and url given in the classroom then read it on my pandas data frame.The codes which i used to obtain this data are;\n",
    "> url=\"\"\n",
    "response =requests.get(url)\n",
    "with open(os.path.join(url.split('/')[-1]), mode='wb') as file:\n",
    "file.write(response.content).\n",
    "\n",
    "Then i read my programmatically downloaded file on my pandas dataframe using pd.read_csv command and since this is a tsv \n",
    "file,i also wrote sep='\\t' under my command.\n",
    "The third data is the additional twitter data. I was to use tweepy library to obtain this data but since i had not received my API keys i opted for the tweet_json.txt file that was provided by the classroom.I downloaded the tweet_json.txt file and saved it in my project folder.On my jupyter notebook i used the following command: \n",
    "> {\"df = []\n",
    "with open ('tweet_json.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = (json.loads(line))\n",
    "        tweet_id = tweet['id']\n",
    "        retweet_count = tweet['retweet_count']\n",
    "        favorite_count = tweet['favorite_count']\n",
    "        df.append({'retweet_count' : retweet_count,\n",
    "                  'favorite_count' : favorite_count,\n",
    "                  'tweet_id' : tweet_id})\"}.\n",
    "                  \n",
    "to obtain data from the txt file and finally loaded it to my pandas dataframe.Thus with all these codes i was able to obtain all my three datasets.\n",
    "    \n",
    "    \n",
    "\n",
    "The next part after gathering my data was to assess my data.Assesing is basically,going through your data to check if its clean, dirty or untidy.There are two ways of assessing your data, that is visual assessment and programmatic assessment.Visual assessment is assessing your data using observation while programmatic assessment is using codes to go through your dataset.I used both types of assessment but the programmatic type was the best since it gives overview of all the data.The codes i used for programmatical assessment are.sample(),.info(),.describe(),.shape,islower(),isnull() and duplicated().These codes helped me identify the quality and tidy issues within the dataset. The quality issues are:\n",
    "1.  Tweet_ids are integers and should be strings\n",
    "1. Missing values in  in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_timestamp,expanded_urls.\n",
    "1. Some dog names are in lower case letters\n",
    "1. Name column has an invalid recordS like 'a'\n",
    "1. Text column can be renamed to tweet.\n",
    "1. Timestamp column are strings\n",
    "1. There are 66 duplicated jpg_url\n",
    "1. Predictions(p1,p2,p3) are inconsistent in that some have lowercase others have uppercase.\n",
    "The tidy issues are:\n",
    "1. Each variable does not form a column,since the dog stages can be combined into one column.\n",
    "1. Merging the three datasets into one master dataset.\n",
    "These were the issues that I obtained from my assessment.\n",
    "\n",
    "Finally, the last part of data wrangling is data cleaning.This is where data is cleaned by removing or working on those quality and tidy issues.I had eight quality issues and two tidyness issues that i cleaned in this section.Here, i used the Define,Code,Test algorithm, where you define whatever you are going to do then execute your codes and finally test it to see if it worked.However, before i started cleaning I had to make a copy of my originaldataset, for example;\n",
    "image_prediction_clean = image_prediction.copy().After copying,I started cleaning my data as follows:\n",
    "- Changing the data type of 'tweet_id' in all three datasets using astype(str).\n",
    "- Removing the columns with missing values using the command .drop().\n",
    "- For consistency purposes, i changed the dog names starting with lower case to upper case letters using the command apply(str.title).\n",
    "- Removing invalid names. First i identified these invalid records and got rid of them using for loop.\n",
    "- Renaming the text column to tweet using the command .rename().\n",
    "- Changing the datatype of timestamp to datetime using the command pd.to_datetime.\n",
    "- Dropping the duplicated jpeg urls using the command .drop_duplicates().\n",
    "- Changing the prediction characters starting with lowercase letter to uppercase letters using the command .apply(str.title).\n",
    "- Combining the four columns doggo,floofer,pupper,puppo to one column called dog_stage.Thus obeying the structural rule of each vvariable should form one column.\n",
    "- Combining all the three datasets to form one master dataset using the command pd.merge.\n",
    "\n",
    "After cleaning my data, i store my combined dataset to a twitter_archive_master.csv file.And that is how i was able to wrangle my data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67349a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
