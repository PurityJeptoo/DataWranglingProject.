{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdfd9dd",
   "metadata": {},
   "source": [
    "# Report on Data Wrangling\n",
    "<P>Data Wrangling is all about gathering,assessing and cleaning data.In this project which is my second project under Udacity, I was able to gather,assess and clean my data set.</P>\n",
    "\n",
    "<P> In this project,we were required to gather data from three different sources. These data are twitter_enhanced_archive.csv,image_prediction.tsv and additional data from twitter.Twitter_enhanced_archive.csv link was provided in the Udacity classroom, thus i downloaded the file manually and stored it in my project folder.This is a csv file which means its separators are commas.On my jupyter notebook, i read my file into a pandas dataframe using the following code \"pd.read_csv('twitter_enhanced_archive.csv) and called it to see if my data is gathered.\n",
    "The second dataset is the image_prediction.tsv data.This is tsv file which means that its delimiters are tabs.I downloaded this data programmatically using the requests library and url given in the classroom then read it on my pandas data frame.The codes which i used to obtain this data are:\n",
    "url=\"\"\n",
    "response =requests.get(url)\n",
    "with open(os.path.join(url.split('/')[-1]), mode='wb') as file:\n",
    "file.write(response.content)\n",
    "Then i read my programmatically downloaded file on my pandas dataframe using pd.read_csv command and since this is a tsv file,i also wrote sep='\\t' under my command.\n",
    "The third data is the additional twitter data. I was to use tweepy library to obtain this data but since i had not received my API keys i opted for the tweet_json.txt file that was provided by the classroom.I downloaded the tweet_json.txt file and saved it in my project folder.On my jupyter notebook i used the following command: \n",
    "\"df = []\n",
    "with open ('tweet_json.txt') as f:\n",
    "    for line in f:\n",
    "        tweet = (json.loads(line))\n",
    "        tweet_id = tweet['id']\n",
    "        retweet_count = tweet['retweet_count']\n",
    "        favorite_count = tweet['favorite_count']\n",
    "        df.append({'retweet_count' : retweet_count,\n",
    "                  'favorite_count' : favorite_count,\n",
    "                  'tweet_id' : tweet_id})\"to obtain data from the txt file and finally loaded it to my pandas dataframe.Thus with all these codes i was able to obtain all my three datasets.</P>\n",
    "\n",
    "<P>The next part after gathering my data was to assess my data.Assesing is basically,going through your data to check if its clean, dirty or untidy.There are two ways of assessing your data, that is visual assessment and programmatic assessment.Visual assessment is assessing your data using observation while programmatic assessment is using codes to go through your dataset.I used both types of assessment but the programmatic type was the best since it gives overview of all the data.the codes i used for programmatical assessment are.sample(),.info(),.describe(),.shape,islower(),isnull() and duplicated().These codes helped me identify the quality and tidy issues within the dataset. The quality issues are:\n",
    "Tweet_ids are integers and should be strings\n",
    "missing values in  in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_timestamp,expanded_urls.\n",
    "some dog names are in lower case letters\n",
    "Name column has an invalid recordS like 'a'\n",
    "text column can be renamed to tweet.\n",
    "Timestamp column are strings\n",
    "There are 66 duplicated jpg_url\n",
    "predictions(p1,p2,p3) are inconsistent in that some have lowercase others have uppercase.\n",
    "The tidy issues are:\n",
    "Each variable does not form a column,since the dog stages can be combined into one column.\n",
    "Merging the three datasets into one master dataset.\n",
    "These were the issues that I obtained from my assessment.</P>\n",
    "\n",
    "<p>Finally, the last part of data wrangling is data cleaning.This is where data is cleaned by removing or working on those quality and tidy issues.I had eight quality issues and two tidyness issues that i cleaned in this section.Here, i used the Define,Code,Test algorithm, where you define whatever you are going to do then execute your codes and finally test it to see if it worked.However, before i started cleaning I had to make a copy of my originaldataset, for example;\n",
    "image_prediction_clean = image_prediction.copy().After copying,I started cleaning my data. That is how i was able to wrangle my data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67349a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
